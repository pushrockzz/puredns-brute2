name: Puredns Bruteforce Worker 

on:
  workflow_dispatch:
    inputs:
      primary_repo_owner:
        description: 'The owner of the primary repository.'
        required: true
        type: string
      primary_repo_name:
        description: 'The name of the primary repository.'
        required: true
        type: string
      primary_run_id:
        description: 'The run ID of the main workflow.'
        required: true
        type: string
      chunk_package_artifact_name:
        description: 'The name of the artifact package.'
        required: true
        type: string
      secondary_matrix_json:
        description: 'The JSON string for the matrix assigned to this worker.'
        required: true
        type: string

permissions:
  contents: write # For checkout
  actions: read   # To read artifacts from another repo

jobs:
  process_assigned_chunks:
    name: Process Assigned Bruteforce Chunks
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        # Use credentials for your container registry
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}
    strategy:
      fail-fast: false
      max-parallel: 20
      # The matrix is populated by the JSON string passed from the primary workflow
      matrix:
        pair: ${{ fromJson(github.event.inputs.secondary_matrix_json && github.event.inputs.secondary_matrix_json || '[]') }}
    steps:
      - name: Display Trigger Payload (Debug)
        run: |
          echo "WORKER: Received payload:"
          echo "${{ toJson(github.event.inputs) }}"
          echo "---"
          echo "WORKER: My assigned chunk for this job instance:"
          echo "${{ toJson(matrix.pair) }}"

      - name: Checkout repository (this worker's repo)
        uses: actions/checkout@v3

      - name: Download Full Chunks Package from Primary Account
        env:
          # This secret must be configured in the repository where this workflow runs
          GH_TOKEN_PRIMARY_ACCOUNT_READ: ${{ secrets.PAT_FOR_PRIMARY_ACCOUNT_READ }}
          PRIMARY_REPO: ${{ github.event.inputs.primary_repo_owner }}/${{ github.event.inputs.primary_repo_name }}
          PRIMARY_RUN_ID: ${{ github.event.inputs.primary_run_id }}
          ARTIFACT_NAME: ${{ github.event.inputs.chunk_package_artifact_name }}
        shell: bash
        run: |
          echo "WORKER: Downloading artifact '$ARTIFACT_NAME' from '$PRIMARY_REPO', run ID '$PRIMARY_RUN_ID'"
          
          # Check for gh CLI and install if not present
          if ! command -v gh &> /dev/null; then
            echo "INFO: gh CLI not found. Installing..."
            (apt-get update -qy && apt-get install -qy curl && \
            curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg && \
            chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg && \
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null && \
            apt-get update -qy && apt-get install -qy gh) || { echo "::error:: gh CLI installation failed."; exit 0; }
          fi

          if ! command -v gh &> /dev/null; then
            echo "::error:: gh CLI is not available after attempting installation."
            exit 0
          fi

          # CRITICAL FIX: Explicitly log in to GitHub using the provided PAT.
          # This ensures the subsequent 'gh' command is properly authenticated.
          echo "$GH_TOKEN_PRIMARY_ACCOUNT_READ" | gh auth login --with-token
          if [ $? -ne 0 ]; then
             echo "::error:: gh auth login failed. The PAT may be incorrect or lack necessary permissions."
             exit 0
          fi

          # Now, this download command will use the authenticated session
          gh run download "$PRIMARY_RUN_ID" -R "$PRIMARY_REPO" -n "$ARTIFACT_NAME" --dir .
          
          PACKAGE_FILENAME="$ARTIFACT_NAME.tar.gz"
          if [ ! -f "$PACKAGE_FILENAME" ]; then
            echo "::error:: Failed to download '$PACKAGE_FILENAME'. The artifact may not exist or the name is incorrect."
            exit 0
          fi
          echo "SUCCESS: Downloaded '$PACKAGE_FILENAME'."

      - name: Extract Bruteforce Package
        shell: bash
        run: |
          PACKAGE_FILENAME="${{ github.event.inputs.chunk_package_artifact_name }}.tar.gz"
          echo "TERTIARY WORKER: Extracting $PACKAGE_FILENAME..."
          tar -xzvf "$PACKAGE_FILENAME"
          if [ ! -d "chunks" ] || [ ! -f "targets.txt" ] || [ ! -f "resolvers-trusted.txt" ]; then
            echo "::error:: 'chunks/', 'targets.txt', or 'resolvers-trusted.txt' missing after extraction."
            exit 0
          fi
          echo "SUCCESS: Extraction complete."

      - name: Install Tools
        run: |
          # Installing smap
          if ! command -v smap >/dev/null; then
            echo "Installing smap…"
            go install -v github.com/s0md3v/smap/cmd/smap@latest
          else
            echo "smap already in cache"
          fi    
          # Installing inscope
          if ! command -v inscope >/dev/null; then
            echo "Installing inscope…"
            go install -v github.com/tomnomnom/hacks/inscope@latest
          else
            echo "inscope already in cache"
          fi    
          
          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install -v github.com/tomnomnom/anew@latest
          else
            echo "anew already in cache"
          fi
          
          if ! command -v cut-cdn >/dev/null; then
            echo "Installing cut-cdn…"
            go install github.com/ImAyrix/cut-cdn@latest
          else
            echo "cut-cdn already in cache"
          fi     

          if ! command -v naabu >/dev/null; then
            echo "Installing naabu…"
            go install -v github.com/projectdiscovery/naabu/v2/cmd/naabu@latest
          else
            echo "naabu already in cache"
          fi

          pip3 install --no-cache-dir ipaddress
          
          echo "$HOME/go/bin" >> $GITHUB_PATH
          
      - name: Resolve Discovered Subdomains with PureDNS
        id: run_puredns
        shell: bash
        run: |
          DOMAIN="${{ matrix.pair.domain }}"
          CHUNK_FILE="${{ matrix.pair.chunk }}"
          PUREDNS_FILE="puredns_file.txt"
          # Define all filenames as requested
          RESOLVED_FILE="puredns_resolved.txt"
          MASSDNS="massdns.txt"
          MASSDNS_FILE="massdns_file.txt"
          TMP_CLEANMASSDNS=$(mktemp)
          
          echo "INFO: Resolving subdomains for '$DOMAIN' using chunk '$CHUNK_FILE'..."
          # The puredns command is now updated with --write-massdns
          cat "$CHUNK_FILE" | puredns bruteforce "$DOMAIN" \
            -r resolvers-trusted.txt \
            --rate-limit 5000 \
            --skip-validation \
            --skip-wildcard-filter \
            --write "$PUREDNS_FILE" \
            --write-massdns "$MASSDNS" \
            --quiet
          
          if [ ! -s "$PUREDNS_FILE" ]; then
            echo "INFO: No subdomains were resolved. Exiting this job."
            # Create empty files to prevent downstream errors
            touch "$RESOLVED_FILE" "$MASSDNS_FILE" "$PUREDNS_FILE"
            exit 0
          fi
          
          echo "✅ PureDNS resolution complete. $(wc -l < "$PUREDNS_FILE") subdomains were successfully resolved."
                 
          wget -qO .scope https://raw.githubusercontent.com/Pcoder7/puredns-bruteforce/refs/heads/main/.scope
          
          echo "Checking scope file for '$DOMAIN'..."
          cat .scope | head -n5
          
          cat "$PUREDNS_FILE" | inscope -s .scope > "$RESOLVED_FILE" || true

          if [ ! -s "$MASSDNS" ]; then
            echo "INFO: MassDNS file is empty. Skipping enrichment."
            touch "$MASSDNS_FILE"
            exit 0
          fi

          awk 'NF { sub(/\.$/,"",$1); print }' "$MASSDNS" > "$TMP_CLEANMASSDNS"

          awk ' \
          {gsub(/\r$/,"");sub(/^[ \t]+/,"");sub(/[ \t]+$/,"")} \
          FNR==NR{if($0)patterns[++c]=$0;next} \
          !setup{regex="";for(i=1;i<=c;i++){regex=regex (i>1?"|":"") "("patterns[i]")"};if(regex=="")regex="^\b$";setup=1} \
          $2=="A" && $1~regex \
          ' .scope "$TMP_CLEANMASSDNS" | anew -q "$MASSDNS_FILE"
          
          echo "✅ MassDNS processing complete. $(wc -l < "$MASSDNS_FILE") in-scope A-records found."
          
          rm -f "$TMP_CLEANMASSDNS" "$MASSDNS"

      - name: Map subdomains to ports with CDN filtering
        id: map_subdomains_cdn
        shell: bash
        run: |
          # This block is nearly identical to your provided logic, ensuring it runs as intended.
          set -e
          trap '' SIGPIPE

          MASSDNS_FILE="massdns_file.txt"
          SMAP_FILE="smap.txt"
          OUTPUT="subdomain_ports.txt"
          
          if [ ! -s "$MASSDNS_FILE" ]; then
            echo "INFO: massdns_file.txt is empty. Skipping port mapping."
            touch "$OUTPUT"
            exit 0
          fi
          
          TMP_IP2SUB=$(mktemp)
          TMP_IP_ONLY=$(mktemp)
          TMP_NONCDN=$(mktemp)
          TMP_CDN=$(mktemp)
          TMP_SMAP_NONCDN=$(mktemp)
          TMP_RUSTSCAN=$(mktemp)

          echo "▶ Cleaning & extracting A‑records from $MASSDNS_FILE…"
          awk '{ print $3, $1 }' "$MASSDNS_FILE" | sort -k1,1 > "$TMP_IP2SUB"

          echo "▶ Pulling unique IPs…"
          cut -d' ' -f1 "$TMP_IP2SUB" | sort -u > "$TMP_IP_ONLY"

          echo "▶ Filtering non‑CDN IPs with cut-cdn…"
          cat "$TMP_IP_ONLY" | cut-cdn -ua -t 50 -silent -o "$TMP_NONCDN"

          
          echo "✅ All done. TMP_IP_ONLY contains $(wc -l < "$TMP_IP_ONLY") IP"
          head -n5 "$TMP_IP_ONLY"

          echo "==================================================================="
          echo "✅ All done. TMP_NONCDN contains $(wc -l < "$TMP_NONCDN") IP"
          head -n5 "$TMP_NONCDN"
          echo "==================================================================="
          
          echo "▶ Deriving CDN IP list…"
          cat "$TMP_IP_ONLY" | anew -d "$TMP_NONCDN" > "$TMP_CDN"
          
          echo "✅ All done. TMP_CDN contains $(wc -l < "$TMP_CDN") IP"
          head -n5 "$TMP_CDN"
          
          echo "==================================================================="
          
          echo "▶ Running rustscan on non‑CDN IPs…"

          smap -iL "$TMP_NONCDN" -oP "$TMP_SMAP_NONCDN" || true
          rustscan -a "$TMP_NONCDN" -p 80,443 --no-banner -t 1000 --tries 1 -u 20000 --scan-order "Random" -b 100 --greppable --accessible > "$TMP_RUSTSCAN" || true
          cat "$TMP_RUSTSCAN" | awk -F ' -> ' '{ gsub(/[\[\]]/, "", $2); n = split($2, p, ","); for(i=1;i<=n;i++) print $1 ":" p[i] }' | anew -q "$TMP_SMAP_NONCDN" || true

          echo "✅ All done. TMP_SMAP_NONCDN contains $(wc -l < "$TMP_SMAP_NONCDN") IP" 
          echo "==================================================================="
          head -n50 "$TMP_SMAP_NONCDN"
          echo "==================================================================="
          
          echo "▶ Merging non‑CDN and CDN IP lists into $SMAP_FILE…"
          cat "$TMP_SMAP_NONCDN" "$TMP_CDN" | sort -u > "$SMAP_FILE"
         
          echo "✅ All done. SMAP_FILE contains $(wc -l < "$SMAP_FILE") IP" 
          echo "==================================================================="
          head -n50 "$SMAP_FILE"
          echo "==================================================================="
          
          echo "▶ Joining with $SMAP_FILE to produce final output..."
          awk -F: '
            NF==2 { print $1, $2 }
            NF==1 { print $1, ""  }
          ' "$SMAP_FILE" \
            | sort -k1,1 \
            | join - "$TMP_IP2SUB" \
            | { 
              awk '
                NF >= 2 { 
                  if (NF == 3 && $2 ~ /^[0-9]+$/) { 
                    print $3 ":" $2 
                  } else { 
                    print $NF 
                  } 
                }
              '       
            } \
            > "$OUTPUT"

          echo "✅ Generated $OUTPUT with $(wc -l < $OUTPUT) entries."

          head -n50 "$OUTPUT"
          
          rm -f "$TMP_IP2SUB" "$TMP_IP_ONLY" "$TMP_NONCDN" "$TMP_CDN" "$TMP_SMAP_NONCDN" "$TMP_RUSTSCAN"

      - name: Sort Resolved Results into Root Domain Folders
        shell: bash
        run: |
          DOMAIN="${{ matrix.pair.domain }}"
          RESOLVED_FILE="puredns_resolved.txt"
          PORTS_INPUT_FILE="subdomain_ports.txt"
        
          mkdir -p "results/$DOMAIN"
          
          if [ -s "$RESOLVED_FILE" ]; then
            echo "  -> Copying puredns results for '$DOMAIN'"
            cp "$RESOLVED_FILE" "results/$DOMAIN/puredns_results.txt"
          fi

          if [ -s "$PORTS_INPUT_FILE" ]; then
            echo "  -> Copying port data for '$DOMAIN'"
            cp "$PORTS_INPUT_FILE" "results/$DOMAIN/subdomain_ports.txt"
          fi

      - name: Create Safe Name for Artifact
        id: safe_name
        shell: bash
        run: |
          # Creates a unique name like 'navy_mil_chunks_wordlist_chunk_0001'
          SAFE_NAME=$(echo "${{ matrix.pair.domain }}_${{ matrix.pair.chunk }}" | tr '/.' '_')
          echo "safe_name=$SAFE_NAME" >> $GITHUB_OUTPUT
      
      - name: Prepare Staging Directory for Upload
        shell: bash
        run: |
          SAFE_NAME="${{ steps.safe_name.outputs.safe_name }}"
          DOMAIN="${{ matrix.pair.domain }}"
          
          # Create a clean directory for this runner's upload
          mkdir -p staging

          # This logic now correctly finds the files created by the previous step
          # in the 'results/$DOMAIN/' directory.
          
          if [ -s "results/$DOMAIN/puredns_results.txt" ]; then
            # Copies the file to a unique name like 'staging/navy_mil_...___puredns.txt'
            cp "results/$DOMAIN/puredns_results.txt" "staging/${SAFE_NAME}___puredns.txt"
          fi
          
          if [ -s "results/$DOMAIN/subdomain_ports.txt" ]; then
            # Copies the file to a unique name like 'staging/navy_mil_...___ports.txt'
            cp "results/$DOMAIN/subdomain_ports.txt" "staging/${SAFE_NAME}___ports.txt"
          fi

      - name: Upload Results Artifact
        uses: actions/upload-artifact@v4
        with:
          # The artifact name itself is still unique, which is good practice
          name: bruteforce-results-primary-${{ steps.safe_name.outputs.safe_name }} # (or -secondary-, -tertiary-)
          # The path now points to our flat staging directory with unique files
          path: staging/
          retention-days: 1

  merge_results:
    name: Merge All Bruteforce Results
    runs-on: ubuntu-latest
    needs: process_assigned_chunks # Or your previous job name
    if: always()
    outputs:
      has_results: ${{ steps.consolidate.outputs.has_results }}    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Download all result artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: 'bruteforce-results-*'
          path: temp-results
          merge-multiple: true

      - name: Consolidate all results
        id: consolidate
        shell: bash
        run: |
          mkdir -p final_results
          
          if [ ! -d "temp-results" ] || [ -z "$(ls -A temp-results)" ]; then
            echo "::warning:: No result artifacts were found to merge."
            echo "has_results=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "INFO: Aggregating all downloaded results..."
          
          for filepath in temp-results/*; do
            filename=$(basename "$filepath")
            
            # This precise regex extracts the domain part before '_chunks'
            domain_part=$(echo "$filename" | sed -E 's/^([^_]+(_[^_]+)*)_chunks.*/\1/')
            
            # Convert the extracted part's underscores to dots
            root_domain=$(echo "$domain_part" | tr '_' '.')

            if [ -n "$root_domain" ]; then
              mkdir -p "final_results/$root_domain"
              
              if [[ "$filename" == *___puredns.txt ]]; then
                cat "$filepath" >> "final_results/$root_domain/puredns_results.txt"
              elif [[ "$filename" == *___ports.txt ]]; then
                cat "$filepath" >> "final_results/$root_domain/subdomain_ports.txt"
              fi
            fi
          done
          
          echo "INFO: De-duplicating all aggregated files..."
          for final_file in $(find final_results -type f -name "*.txt"); do
              sort -u -o "$final_file" "$final_file"
          done

          if [ -n "$(ls -A final_results)" ]; then
            echo "✅ Successfully consolidated results."
            echo "has_results=true" >> $GITHUB_OUTPUT
          else
            echo "::warning:: Consolidation resulted in no final data."
            echo "has_results=false" >> $GITHUB_OUTPUT
          fi

          echo "Final directory structure:"
          ls -R final_results
          
      - name: Upload Final Consolidated Artifact
        uses: actions/upload-artifact@v4
        with:
          name: final-bruteforce-results
          path: final_results/
          retention-days: 1
 
  commit_results:
    name: Commit Puredns Bruteforce Results
    needs: merge_results
    if: needs.merge_results.outputs.has_results == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Download the final consolidated bruteforce results
        uses: actions/download-artifact@v4
        with:
          name: final-bruteforce-results
          path: final_results
  
      - name: Organize and Push to store-recon    
        shell: bash
        env:
          STORE_RECON_PAT: ${{ secrets.PAT_FOR_SECONDARY_ACCOUNT_REPO }}
          ACCOUNT2_USERNAME: ${{ secrets.ACCOUNT2_REPO_OWNER }}
          STORE: ${{ secrets.STORE }}
          CORRELATION_ID: ${{ github.event.inputs.primary_run_id }}
        run: |
          set -e
          RESULTS_DIR="${GITHUB_WORKSPACE}/final_results"
          if [ ! -d "$RESULTS_DIR" ] || [ -z "$(ls -A "$RESULTS_DIR")" ]; then
            echo "::warning:: Results directory is empty. Nothing to commit."
            exit 0
          fi
          
          echo "Cloning ${{ env.STORE }} to commit results..."
          git config --global user.name "Secondary-Puredns-Bruteforce Bot"
          git config --global user.email "actions-bot@users.noreply.github.com"
          
          TMP_DIR="$(mktemp -d)"
          git clone "https://x-access-token:${STORE_RECON_PAT}@github.com/${ACCOUNT2_USERNAME}/${STORE}.git" "$TMP_DIR"
          cd "$TMP_DIR"
          
          echo "Merging new consolidated results into the repository..."
          for domain_dir in "${RESULTS_DIR}"/*; do
            if [ ! -d "$domain_dir" ]; then continue; fi
            
            domain_name=$(basename "$domain_dir")
            dest_repo_dir="results/$domain_name"
            mkdir -p "$dest_repo_dir"
            
            # --- MERGE BLOCK 1 ---
            source_puredns_file="$domain_dir/puredns_results.txt"
            dest_all_subs_file="$dest_repo_dir/all_subdomains.txt"
            if [ -s "$source_puredns_file" ]; then
              # FIX: Sanitize the file to remove NULL characters.
              echo "  -> Sanitizing puredns results for '$domain_name'..."
              tr -d '\0' < "$source_puredns_file" > "$source_puredns_file.tmp" && mv "$source_puredns_file.tmp" "$source_puredns_file"

              echo "  -> Merging puredns results into '$dest_all_subs_file'"
              temp_merged_file_1=$(mktemp)
              if [ -f "$dest_all_subs_file" ]; then
                cat "$source_puredns_file" "$dest_all_subs_file" | sort -u > "$temp_merged_file_1"
              else
                sort -u "$source_puredns_file" > "$temp_merged_file_1"
              fi
              mv "$temp_merged_file_1" "$dest_all_subs_file"
            fi
            
            # --- MERGE BLOCK 2 ---
            source_ports_file="$domain_dir/subdomain_ports.txt"
            dest_puredns_file="$dest_repo_dir/puredns_result.txt"
            if [ -s "$source_ports_file" ]; then
              # FIX: Sanitize the file to remove NULL characters.
              echo "  -> Sanitizing port data for '$domain_name'..."
              tr -d '\0' < "$source_ports_file" > "$source_ports_file.tmp" && mv "$source_ports_file.tmp" "$source_ports_file"

              echo "  -> Merging port data into '$dest_puredns_file'"
              temp_merged_file_2=$(mktemp)
              if [ -f "$dest_puredns_file" ]; then
                cat "$source_ports_file" "$dest_puredns_file" | sort -u > "$temp_merged_file_2"
              else
                sort -u "$source_ports_file" > "$temp_merged_file_2"
              fi
              mv "$temp_merged_file_2" "$dest_puredns_file"
            fi
          done
          
          # Stage changes before checking for a diff.
          git add results/

          # Check if there are any staged changes.
          if git diff --cached --quiet; then
            echo "No new unique data found to commit."
            exit 0
          fi

          # --- RETRY LOOP FOR COMMIT AND PUSH ---
          MAX_ATTEMPTS=5
          for (( i=1; i<=MAX_ATTEMPTS; i++ )); do
            echo "[Attempt $i/$MAX_ATTEMPTS] Committing and pushing changes from secondary worker..."
            git pull --rebase origin main
            git commit -m "feat: Puredns bruteforce assets from Correlation ID: ${CORRELATION_ID}" || true
            if git push -v origin main; then
              echo "✅ Successfully pushed new bruteforce results to ${{ env.STORE }} on attempt $i."
              exit 0
            fi
            echo "::warning:: Push failed on attempt $i. Retrying after a short delay..."
            if [ $i -eq $MAX_ATTEMPTS ]; then echo "::error:: All $MAX_ATTEMPTS push attempts failed."; exit 0; fi
            sleep $(( 5 * i ))
          done
